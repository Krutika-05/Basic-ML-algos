{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from my_evaluation import my_evaluation\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class my_evaluation:\n",
    "    # Binary class or multi-class classification evaluation\n",
    "    # Each data point can only belong to one class\n",
    "\n",
    "    def __init__(self, predictions, actuals, pred_proba=None):\n",
    "        # inputs:\n",
    "        # predictions: list of predicted classes\n",
    "        # actuals: list of ground truth\n",
    "        # pred_proba: pd.DataFrame of prediction probability of belonging to each class\n",
    "        self.predictions = np.array(predictions)\n",
    "        self.actuals = np.array(actuals)\n",
    "        self.pred_proba = pred_proba\n",
    "        if type(self.pred_proba)!=type(None):\n",
    "            self.classes_ = list(self.pred_proba.keys())\n",
    "        else:\n",
    "            self.classes_ = list(set(list(self.predictions)+list(self.actuals)))\n",
    "    \n",
    "        print(self.classes_)\n",
    "        self.confusion_matrix = None\n",
    "\n",
    "    def confusion(self):\n",
    "        # compute confusion matrix for each class in self.classes_\n",
    "        # self.confusion_matrix = {self.classes_[i]: {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}}\n",
    "        # no return variables\n",
    "        # write your own code below\n",
    "\n",
    "        print(\"insidde confusion\")\n",
    "        correct = self.predictions == self.actuals\n",
    "        wrong = self.predictions != self.actuals\n",
    "        self.acc = float(Counter(correct)[True])/len(correct)\n",
    "        self.confusion_matrix = {}\n",
    "        \n",
    "        for label in self.classes_:\n",
    "            tp=0\n",
    "            fp=0\n",
    "            tn=0\n",
    "            fn=0\n",
    "            #print(\"inside first for\")\n",
    "            #print(label)\n",
    "            \n",
    "            \n",
    "            for i in range(len(self.predictions)):\n",
    "                #print(\"inside 2,d for\")\n",
    "            #print(self.classes_)\n",
    "            #tp_all = (Counter(correct))\n",
    "                if self.actuals[i]==label and label==self.predictions[i]:\n",
    "                    #print(\"inside if\")\n",
    "                    tp=tp+1\n",
    "                    #print(tp)\n",
    "            #tp=dict((k,tp_all[k]) for k in[True])\n",
    "            #print(\"tp: \")\n",
    "            #print(tp)\n",
    "            \n",
    "            #fp_all = (Counter(wrong))\n",
    "            #fp=dict((k,tp_all[k]) for k in[False])\n",
    "                elif  self.predictions[i]==label and label!=self.actuals[i]:\n",
    "                    fp=fp+1\n",
    "            #fp=val\n",
    "            #print(\"fp: \")\n",
    "            #print(fp)\n",
    "            \n",
    "            #fn_all = (Counter(wrong))\n",
    "            #fn=dict((k,fn_all[k]) for k in[True])\n",
    "                elif self.actuals[i]==label and self.predictions[i]!=label:\n",
    "                    fn=fn+1\n",
    "            #fn=val\n",
    "            #print(\"fn: \")\n",
    "            #print(fn)\n",
    "            #fn = Counter(self.actuals)[label]\n",
    "            #fn_all = dict((k,tp_all[k]) for k in[False])\n",
    "            \n",
    "            #print(fn)\n",
    "                elif self.actuals[i]!=label and label!=self.predictions[i]:\n",
    "                    tn=tn+1\n",
    "                \n",
    "                self.confusion_matrix[label] = {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}\n",
    "                #print(tp)\n",
    "                #print(fp)\n",
    "                #print(tn)\n",
    "                #print(fn)\n",
    "            #print(\"---------------------------------------------------------------------------------\")  \n",
    "        print(\"confusion matrix:\")\n",
    "        print(self.confusion_matrix)\n",
    "            #tn = len(self.actuals) - fn\n",
    "            #tn= dict((k,fn_all[k]) for k in[False])\n",
    "            #tn=val\n",
    "            #print(\"tn: \")\n",
    "            #print(tn)\n",
    "            \n",
    "\n",
    "        #set_trace()\n",
    "        return   \n",
    "    \n",
    "    def accuracy(self):\n",
    "        if self.confusion_matrix==None:\n",
    "            self.confusion()\n",
    "        return self.acc\n",
    "\n",
    "    def precision(self, target=None, average = \"macro\"):\n",
    "        # compute precision\n",
    "        # target: target class (str). If not None, then return precision of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average precision\n",
    "        # output: prec = float\n",
    "        # note: be careful for divided by 0\n",
    "\n",
    "        if self.confusion_matrix==None:\n",
    "            self.confusion()\n",
    "        if target in self.classes_:\n",
    "            tp = self.confusion_matrix[target][\"TP\"]\n",
    "            fp = self.confusion_matrix[target][\"FP\"]\n",
    "            if tp+fp == 0:\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = float(tp) / (tp + fp)\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                prec = self.accuracy()\n",
    "            else:\n",
    "                prec = 0\n",
    "                n = len(self.actuals)\n",
    "                for label in self.classes_:\n",
    "                    tp = self.confusion_matrix[label][\"TP\"]\n",
    "                    fp = self.confusion_matrix[label][\"FP\"]\n",
    "                    if tp + fp == 0:\n",
    "                        prec_label = 0\n",
    "                    else:\n",
    "                        prec_label = float(tp) / (tp + fp)\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"Unknown type of average.\")\n",
    "                    prec += prec_label * ratio\n",
    "        #print(\"precision\")\n",
    "        #print(prec)\n",
    "        return prec\n",
    "\n",
    "    def recall(self, target=None, average = \"macro\"):\n",
    "        # compute recall\n",
    "        # target: target class (str). If not None, then return recall of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average recall\n",
    "        # output: recall = float\n",
    "        # note: be careful for divided by 0\n",
    "        if target==None:\n",
    "            rec=0\n",
    "            \n",
    "            \n",
    "        #referred the code for precision written by professor.\n",
    "        #recall and precision values for micro macro and average are calculated similarly.\n",
    "        #In case of recall, tp and fn are used.\n",
    "        if target in self.classes_:\n",
    "            tp = self.confusion_matrix[target][\"TP\"]\n",
    "            fn = self.confusion_matrix[target][\"FN\"]\n",
    "            if tp+fn == 0:\n",
    "                rec = 0\n",
    "            else:\n",
    "                rec = float(tp) / (tp + fn)\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                rec = self.accuracy()\n",
    "            else:\n",
    "                rec = 0\n",
    "                n = len(self.actuals)\n",
    "                for label in self.classes_:\n",
    "                    tp = self.confusion_matrix[label][\"TP\"]\n",
    "                    fn = self.confusion_matrix[label][\"FN\"]\n",
    "                    if tp + fn == 0:\n",
    "                        rec_label = 0\n",
    "                    else:\n",
    "                        rec_label = float(tp) / (tp + fn)\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"Unknown type of average.\")\n",
    "                        rec += rec_label * ratio\n",
    "        #print(\"recall\")\n",
    "        #print(rec)\n",
    "        return rec\n",
    "\n",
    " \n",
    "    def f1(self, target=None, average = \"macro\"):\n",
    "        # compute f1\n",
    "        # target: target class (str). If not None, then return f1 of target class\n",
    "        # average: {\"macro\", \"micro\", \"weighted\"}. If target==None, return average f1\n",
    "        # output: f1 = float\n",
    "        #prec = self.precision() \n",
    "        #rec = self.recall()\n",
    "        #f1_score =  2*((prec*rec)/(prec+rec))\n",
    "        #if target=None:\n",
    "            #f1=0\n",
    "            \n",
    "        if target in self.classes_:\n",
    "            prec=self.precision(target,average)\n",
    "            rec=self.recall(target,average)\n",
    "            #tp = self.confusion_matrix[target][\"TP\"]\n",
    "            #fn = self.confusion_matrix[target][\"FN\"]\n",
    "            #fp = self.confusion_matrix[target][\"FP\"]\n",
    "            if prec+rec == 0:\n",
    "                f1 = 0\n",
    "            else:\n",
    "                f1 = 2 * ((prec * rec) / (prec + rec))\n",
    "        else:\n",
    "            if average == \"micro\":\n",
    "                f1 = self.accuracy()\n",
    "                #rec=self.accuracy()\n",
    "            else:\n",
    "                #rec = 0\n",
    "                #prec=0\n",
    "                f1=0\n",
    "                n = len(self.actuals)\n",
    "                for label in self.classes_:\n",
    "                    prec=self.precision(label,average)\n",
    "                    rec=self.recall(label,average)\n",
    "                    #tp = self.confusion_matrix[label][\"TP\"]\n",
    "                    #fn = self.confusion_matrix[label][\"FN\"]\n",
    "                    if prec + rec == 0:\n",
    "                        f1_label = 0\n",
    "                    else:\n",
    "                        f1_label = 2 * ((prec * rec) / (prec + rec))\n",
    "                    if average == \"macro\":\n",
    "                        ratio = 1 / len(self.classes_)\n",
    "                    elif average == \"weighted\":\n",
    "                        ratio = Counter(self.actuals)[label] / float(n)\n",
    "                    else:\n",
    "                        raise Exception(\"Unknown type of average.\")\n",
    "                        f1 += f1_label * ratio\n",
    "        #f1 = 2 * ((prec * rec) / (prec + rec))\n",
    "        #print(f1_score)\n",
    "        return f1\n",
    "\n",
    "\n",
    "        \n",
    "    def auc(self, target):\n",
    "        # compute AUC of ROC curve for each class\n",
    "        # return auc = {self.classes_[i]: auc_i}, dict\n",
    "        if type(self.pred_proba)==type(None):\n",
    "            return None\n",
    "        else:\n",
    "            if target in self.classes_:\n",
    "                order = np.argsort(self.pred_proba[target])[::-1]\n",
    "                #print(order)\n",
    "                tp = self.confusion_matrix[target][\"TP\"]\n",
    "                fp = self.confusion_matrix[target][\"FP\"]\n",
    "                fn = Counter(self.actuals)[target]\n",
    "                tn = len(self.actuals) - fn\n",
    "                tpr = tp/(tp+fn)\n",
    "                fpr = fp/(fp+tn)\n",
    "                auc_target = 0\n",
    "                for i in order:\n",
    "                    pass\n",
    "                    #if self.actuals[i] == target:\n",
    "                        #tp = \"write your own code\"\n",
    "                        #fn = \"write your own code\"\n",
    "                        #tpr = \"write your own code\"\n",
    "                    #else:\n",
    "                        #fp = \"write your own code\"\n",
    "                        #tn = \"write your own code\"\n",
    "                        #pre_fpr = fpr\n",
    "                        #fpr = \"write your own code\"\n",
    "                        #auc_target = \"write your own code\"\n",
    "            else:\n",
    "                raise Exception(\"Unknown target class.\")\n",
    "\n",
    "            return auc_target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
      "insidde confusion\n",
      "confusion matrix:\n",
      "{'Iris-setosa': {'TP': 45, 'TN': 90, 'FP': 0, 'FN': 0}, 'Iris-versicolor': {'TP': 44, 'TN': 85, 'FP': 5, 'FN': 1}, 'Iris-virginica': {'TP': 40, 'TN': 89, 'FP': 1, 'FN': 5}}\n",
      "result\n",
      "{'Iris-setosa': {'prec': 1.0, 'recall': 1.0, 'f1': 1.0, 'auc': 0}, 'Iris-versicolor': {'prec': 0.8979591836734694, 'recall': 0.9777777777777777, 'f1': 0.9361702127659575, 'auc': 0}, 'Iris-virginica': {'prec': 0.975609756097561, 'recall': 0.8888888888888888, 'f1': 0.9302325581395349, 'auc': 0}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from my_evaluation import my_evaluation\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from pdb import set_trace\n",
    "\n",
    "\n",
    "# Load training data\n",
    "data_train = pd.read_csv(r\"C:\\Users\\Shrunali\\DSCI-633\\assignments\\data\\Iris_train.csv\")\n",
    "\n",
    "# Separate independent variables and dependent variables\n",
    "independent = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "X = data_train[independent]\n",
    "y = data_train[\"Species\"]\n",
    "\n",
    "# Fit model\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on training data\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(X)\n",
    "probs = pd.DataFrame({key: probs[:, i] for i, key in enumerate(clf.classes_)})\n",
    "\n",
    "\n",
    "# Evaluate results\n",
    "metrics = my_evaluation(predictions, y, probs)\n",
    "result = {}\n",
    "for target in clf.classes_:\n",
    "    result[target] = {}\n",
    "    metrics.precision(target)\n",
    "    result[target][\"prec\"] = metrics.precision(target)\n",
    "    result[target][\"recall\"] = metrics.recall(target)\n",
    "    result[target][\"f1\"] = metrics.f1(target)\n",
    "    result[target][\"auc\"] = metrics.auc(target)\n",
    "print(\"result\")\n",
    "print(result)\n",
    "#f1 = {average: metrics.f1(target=None, average=average) for average in [\"macro\", \"micro\", \"weighted\"]}\n",
    "#print(\"Average F1 scores: \")\n",
    "#print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
